# Crypto-Price-Prediction-

MODELS USED (by order) :


—	Extreme Gradient Boosting Regressor (XGB) : Extreme Gradient Boosting is basically an efficient implementation of the gradient boosting algorithm (mentioed below). It is called extreme because of the parallelization of tree construction using all of CPU cores during training, distributed Computing for training very large models using a cluster of machines, out-of-Core Computing for very large datasets that don’t fit into memory and cache optimization of data structures and algorithm to make best use of hardware

—	Tensorflow. TensorFlow is an open-source machine learning framework developed by Google. It is a library for building and training machine learning models. It is designed to be flexible, scalable and portable, making it suitable for a wide range of applications.The TensorFlow model works by defining a computational graph that represents the mathematical operations required to perform the desired prediction. This graph is created using TensorFlow operations and tensors, which are multi-dimensional arrays. The model is then trained using gradient descent optimization algorithms to minimize a specified loss function.During training, the input data is fed into the graph and the model updates the weights and biases of the operations in the graph based on the computed gradients. The final prediction is made by passing the input data through the trained graph.TensorFlow also provides a high-level API, Keras, which simplifies the creation and training of models. This makes it easier for developers to get started with TensorFlow and quickly build and train machine learning models for a wide range of applications.

—	PCA. Principal Component Analysis (PCA) is a dimensionality reduction technique that aims to project high-dimensional data onto a lower-dimensional subspace while retaining as much information as possible. It works by transforming the original data into a new set of uncorrelated variables, called principal components, which capture the most significant variations in the data. The first principal component captures the largest variance, the second one captures the second largest variance, and so on. PCA uses singular value decomposition (SVD) to calculate the eigenvectors and eigenvalues of the covariance matrix of the data. The eigenvectors, which are the directions in the data where the variance is maximum, are used as the new axes of the transformed data. The eigenvalues determine the magnitude of the information carried by each of the new axes. The new axes are arranged in decreasing order of importance, and the user can decide how many of them to keep to represent the data. This reduces the dimensionality of the data, making it easier to visualize and analyze. In summary, PCA helps to identify patterns in data, and it can be used for feature extraction, data compression, and visualization.

—	LightGBM. LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be scalable and efficient for large datasets, making it suitable for use with big data. It is a gradient boosting framework that focuses on decision tree learning and is known for its speed and accuracy compared to other gradient boosting algorithms. The model works by combining the outputs from multiple trees to form a prediction. It uses a gradient descent optimization algorithm to minimize the loss function and improve the accuracy of the model. During each iteration, the model updates the weights of the trees based on the gradient of the loss function. The final prediction is made by taking the weighted average of the predictions from all the trees in the model.LightGBM also implements a number of techniques to improve the training speed and reduce memory usage, such as histogram-based binning and leaf-wise tree growth. These features make LightGBM a popular choice for many machine learning applications, particularly in the field of large-scale structured data prediction.

—	Decision Tree Regressor (tree) : Of course, we will start with one of the most basic models. A Decision Tree Regressor is a machine learning model used for regression tasks, where the goal is to predict a continuous target variable. It works by creating a tree-based model that splits the data into multiple subgroups based on the features. Each internal node in the tree represents a feature test that splits the data into two or more subgroups, and each leaf node represents a prediction for the target variable. The prediction for a sample is obtained by traversing the tree from the root to a leaf node based on the feature values of the sample. The algorithm starts with all the data at the root node, and it splits the data into smaller subsets based on the feature that provides the highest reduction in variance for the target variable. The process continues recursively for each child node until a stopping criterion is met, such as a minimum number of samples in a leaf node, or a maximum depth of the tree. The prediction for a new sample is obtained by traversing the tree and finding the corresponding leaf node. The prediction is the average target value of the samples in that leaf node.
In summary, Decision Tree Regression is a simple and interpretable algorithm that can capture complex non-linear relationships between features and target variable. It is computationally efficient and can handle both linear and non-linear relationships in the data. 

—	Gradient Boosting Regressor (GB). Gradient Boosting Regressor is a machine learning model used for regression tasks, where the goal is to predict a continuous target variable. It is an ensemble learning technique that combines multiple weak models to create a stronger model. The algorithm starts with a simple base model, such as a decision tree, and trains it on the data. The residuals, which are the differences between the true target values and the predictions of the base model, are then used to train the next model. The process continues iteratively, with each new model trying to correct the errors of the previous models. The final prediction is obtained by taking a weighted sum of the predictions of all the individual models. The weights are determined by minimizing the loss function, which measures the differences between the true target values and the predictions. Gradient Boosting Regression uses gradient descent to optimize the weights, hence the name "Gradient Boosting". It also uses regularization techniques to prevent overfitting, such as shrinkage, which reduces the learning rate, and early stopping, which stops the training process when the performance on a validation set starts to deteriorate. In summary, Gradient Boosting Regression is a powerful and flexible machine learning model that can handle complex non-linear relationships in the data. It is computationally expensive, but it provides good performance on a wide range of regression tasks.


All our prediction are based on BTC which is the most liquid asset in the crypto universe. This ischosen to have the most robust crypto and build model prediction around solid data. This could totally be on other tokens.

Refer to the notebook to see the results.
